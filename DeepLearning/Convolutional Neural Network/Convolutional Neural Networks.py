# Convolutional Neural Network

# Importing the libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
tf.__version__

# Part 1 - Data Preprocessing
#The first thing we'll do is we will apply some transformations on all the images of the training set. The images of the training set only. We won't apply these same transformations on the test set. The reason why we want to apply some transformations on the images of the training set is for only one purpose. It is to avoid over fitting. Indeed, if we don't apply these transformations well, when training our CNN on the training set we will get a huge difference between the accuracy on the training set and the one on the test set you know, on the evaluation set, actually, we will get very high accuracies on the training set, you know, close to 98% and much lower accuracies on the test set and that is called over fitting and that's something we absolutely need to avoid anyway you know, whether you're working on a classic data set or working for computer vision and for computer vision Well, the way to avoid over fitting isas I said, to apply transformations. 
# So the what, what are these transformations is, well, some simple geometrical transformations or some zooms or some rotations on your images. So basically we're gonna apply some geometrical transformations like transactions to shift some of the pixels. Then we're gonna rotate a bit the images. We're gonna do some horizontal flips. We're gonna do some zoom in and zoom out. Well, you know we're gonna apply a series of transformation so as to modify the images and get them, as we say, augmented. In fact, the technical term of what we're gonna do now, you know, with all these transformations is called Image Augmentation which consists basically of transforming your images of the training set so that your CNN model doesn't overlearn, you know, is not overtrained on the existing images because by applying these transformations we will get new images which is the reason why we call this Image Augmentation. We basically augment the variety, you know, the diversity of the training set images.
# we will create different batches of actually 32 images and these images will either be the original ones or you know, the augmented ones, the transformed ones after we apply the transformations and speaking of applying these transformations well, we're gonna do that exactly with this image data generator class for which we'll find all the arguments here. And you know, most of them correspond to different transformations.I can already tell you that we will use the zoom range  which consists of zooming in or zooming out on the images. We'll also we'll use the horizontal flip which consists of flipping the images horizontally and then we will also use this one, the sheer range which is some kind of transaction. You can check it online but no need to understand this in all the details just know that it's a geometrical transformation
# why do we use these transformations? Well, I'll be honest with you, the reason I'm using them is because I simply took, you know, the code snippet example from Keras website  which is right below exactly here. This is the code snippet example using the image data generator class and as you can see, we use a sharing transformation, a zoom transformation, and a horizontal flip transformation and we're just gonna do the same but of course, feel free to try some other transformations.
# So back into our implementation, Well, let's base that right here and this, as you can see, creates an object which we call train data gen of the image data generated class. So train data gen is an instance of that image data generator class, and which represents of course, the tool that will apply all the transformations on the images of the training set and there is one I haven't mentioned. You know, I mentioned and explained these three ones which are the transformations, but we also noticed this one rescale equals one divided by 255. Can you guess what this is about? You know, we already saw this many times on our classic data sets. Well, this is of course about feature scaling. This will apply feature scaling to each and every single one of your pixels by dividing their value by 255 because remember that each pixel takes a value between zero and 255. So by dividing all of them by 255 we indeed get all pixel values between zero and one which is just like a normalization and once again, feature scaling is absolutely compulsory for neural networks. You know, when training neural networks. All right, so basically this is future scaling and these are the transformations that will perform image augmentation on the images of the training set and this I remind, is in order to prevent over fitting in the end, you can try, actually, you know the future training will have without these and you will see what I mean by over fitting.
train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

#So far this is just the object his next code here that will actually import the training set by accessing it from, you know, our directory and at the same time creating these batches and resizing the images, you know, in case we need to resize them in order to reduce the computations of the machine, you know, to make it less computer intensive, which is what we'll do because we will see that with a lower size we'll still get amazing results in the end.
# Now, next argument, target size. That's indeed the final size of your images. One day, you know will be fed into the Convolution Neural Network and actually I tried with 150 by 150 and that's actually made the training very, very long. So I actually wanted to reduce that to, you know, 64 by 64 and that's totally fine. This will make the training much faster and still we will have amazing results. You'll see that at the end. Then the batch size is, you know, the size of the batches meaning how many images we want to have in each batch and the 32 is a classic default value and we're gonna keep that. That will be totally fine and finally, we have to specify the class mode which is either binary or category call and of course, since now we have a binary outcome, you know, Cat or Dog well, we have to choose, of course, class mode equals binary, but if we had for example more categories like dogs, cats, wolf and fish then we would shoose category 
training_set = train_datagen.flow_from_directory('dataset/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
#now we're gonna move on to the next step, prepossessing to test it and of course, in the spirit of always be as much efficient as we can, well, we're gonna go back to our Keras API and we're just gonna take this time this line of code to, you know get that same image data generator, object to, you know, apply the transformations to the test images. But be careful we're not gonna apply the same transformations here such as the shearing, the zoom, and the horizontal flip, because of course we don't want to touch the test images because they're like new images like when deploying our model in production and therefore of course we have to keep them intact like the original ones. However, what we have to do to them is indeed to rescale their pixels and that's the same as before, you know, remember when we were applying feature scaling to our training set and test set? Well, we used the fit transfer method on the training set but only the transform method on the test set and that was of course to avoid information leakage from the test set and while here that's exactly the same. We have to keep the images of the test set intact by not applying any transformation. However, we have to feature scale them because once again the future predict method of the CNN will have to be applied to the same scaling as the one that was applied on the training set.
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('dataset/test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')


# Part 2 - Building the CNN

#All right, so it's actually gonna start the same as with our artificial neural network because a convolution neural network is still a sequence of layers. Therefore, we're gonna initialize here our CNN with the same class, which remember is sequential class. And that's our first step where we're gonna not only call that sequential class, but mostly create that CNN variable, which will represent exactly this convolutional neural network
# Initialising the CNN
cnn = tf.keras.models.Sequential()

#And now step by step, we're going to use this, you know add method to add to different layers whether they are convolution layers or fully connected layers, and in the end the output layer

# Step 1 - Convolution
#And inside we have to input, 3 important parameters first one is filters, which basically is the number of feature detectors you want to apply to your images, you know, to detect features
#  Then we will also specify a kernel size. And actually I've prepared some slides that I made to show you. So basically, this is the filter. And this filters parameter will tell us how many feature detectors we want, how many filters we want and the kernel size is exactly, you know, the size of that feature detector, meaning the number of rows which is also the number of columns because it is usually a squared array, you know? So for example, if we choose a size of three which is actually the size we're gonna choose well that means that the size of our feature detector will be 3 X 3. And then we have some other arguments, but no worries we'll just keep the default value for the rest of them because that will be just fine. However, of course, we're not going to keep the default value of that activation parameter, which corresponds of course to the activation function, because indeed, you know on a general rule, as long as we haven't reached the output layer we rather want to get a rectifier activation function. And therefore, for this one, activation, we're gonna choose once again the ReLU parameters name which corresponds to the rectifier activation function. And finally, we have one less argument we have to input in which is not displayed here because that's hidden by this one, but which you can actually see here. That's the input shape when you add your very first layer whether it is a convolution layer or dense layer while you have to specify the input shape of your inputs. And here, since we're working with colored images therefore in three dimensions, you know corresponding to the RGB code of colors. And since we actually resized in parts one, our images down to 64 x 64, well the input shape of our images will be 64, 64, and 3. And if we were working with black and white images here we would have one instead of three. But we are working with color images and therefore that's what will be our input shape. 64, 64, and 3
cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))


# Step 2 - Pooling
#And more specifically, we're gonna apply max pooling, just as you saw in the intuition lectures. All right, so once again, we need to take our CNN object from which we're gonna call now a new method but is that really new? According to you, do we have to call the add method again? Well, yes, we do, actually, we're adding the pooling layers to our convolution layer, you know as the next step in the sequence of layers. So there we go. We call the add method once again, and then inside, well that max pooling layer will once again be created as an object, you know, as an instance of a certain class which is called the max pool to the class. And inside we have to specify two essential arguments which are the pool size in this example 2, okay, and well, you know, speaking of 2 that's exactly the pool size, which we're gonna choose and which is the one I recommend to work with when applying, max pooling
# next parameter is stride and we will set value of 2 which mean that we are sliding 2X2 through feature map when we create pooled feature map
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# Adding a second convolutional layer
# now we add second convolutional layer just the same, but we just need to remove that input shape parameter because this one is entered only when you add your very first layer, you know, to automatically connect that first layer to the input layer, which automatically adds the input layer. But here we are already with our second convolutional layer so that's all good, we can just remove it
cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# Step 3 - Flattening
#we can now move on to step three, flattening, which will consist of, of course, flattening the result of all these convolutions and poolings into a one dimensional vector, which will become the input of a future fully connected neural network
cnn.add(tf.keras.layers.Flatten())


# Step 4 - Full Connection
# First, we create a new code cell. Then we take once again our CNN neural network from which we're gonna call the add method because now we're about to add a new layer which is a fully connected layer, and which belongs still to that TF CARES layers, which I've already copied. So I can just base that here. And this time take that dense class. Perfect. And now into some new parenthesis. I'm sure you figured out what to enter as parameters. First, remember we have the units, which is number of hidden neurons you want to have into this new fully connected layer. And since now we're actually dealing with a more complex problem. You know, computer vision is way more complex than data mining classic data set as before, well we're gonna choose a larger number of hidden neurons. We're gonna choose 128 hidden neurons. However, if you picked the same number as before you know, in the previous section with our A and N, well, I'm sure that's totally fine and I'm sure you'll get great results afterwards. But we might get a better accuracy in the end with a larger number of neurons. And therefore, let's choose here. Units equals 128. Great. And now second argument, that's of course the activation function. And once again, my recommendation is that as long as you haven't reached the final output layer I recommend to use a rectifier activation function. And that's exactly what we're gonna specify here with this activation parameter. And remember, the code name for the rectify activation function is ReLU. All right, and perfect. This add, our fully connected layer.
cnn.add(tf.keras.layers.Dense(units=128, activation='relu')) 

# Step 5 - Output Layer
#And finally the step 5. You see how we're being so efficient here in that step 5 we need to add the final output layer which will still be fully connected to that previous hidden layer. We are doing binary classification, therefore we only need 1 neuron to encode that binary class 0 or 1 or you know, cat or dog. And therefore we only need 1 neuron. And for the activation function remember that for the output layer, it is not recommended to have a rectifier activation function but rather a Sigmoid activation function. And that's because of course we're doing binary classification. Otherwise, if we were doing multi-class classification we would have remembered a Softmax activation function. But there we go. This will add a great output layer which will optimize the results in the end
cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

# Part 3 - Training the CNN
# previously we built the brain, which contained the eyes of the AI, you know thanks to the convolutional layers, and now we're gonna make that brain smart with that training of the CNN on all our training set images. And at the same time, as you see, we're gonna evaluate our same model on the test set over the epochs, you know, we're gonna train our CNN over 25 epochs, and at each epoch we will actually see how our model is performing on our test set images. So this is a different kind of training as what we did before because we always used to separate the training and the evaluation, but here this will happen at the same time. That's because we're doing some specific application, which is computer vision.

# Compiling the CNN
#we're gonna compile the CNN, meaning we're gonna connect it to an optimizer, a loss function, and some metrics. And well, you know, here once again we're doing binary classification. And so, very simply, we're gonna compile our CNN exactly the same way as how we compiled our ANN in the previous section. Because indeed, we're gonna choose once again an atom optimizer to you know, perform stochastic gradient descent, to update the weight in order to reduce the loss error between the predictions and the target. Then we're gonna choose the same loss, you know the binary cross entropy loss. Once again, because we're doing exactly the same task, binary classification. And then same for the metrics, we're gonna choose the accuracy metrics, because that's, you know, the most relevant way to measure the performance of a classification model. which is exactly the case with our CNN. And therefore, to compile the CNN, well, it's gonna be a piece of cake, because indeed we're gonna do exactly the same as before. So we're gonna start by taking our CNN, from which we're gonna call the compile method, which will take as input, well, first our optimizer, which we'll choose to be the atom optimizer. Then the loss function, which we'll choose to be the binary cross entropy. All right, and finally, final argument, the metrics, we will choose only one, but remember we have the choice to take several of them, but just the accuracy is fine
cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Training the CNN on the Training set and evaluating it on the Test set
#now to train the CNN on the training set, and evaluating it at the same time on the test set, while it will not be exactly the same as before but once again, very, very similar The fit method, which as always will train the CNN on the training set. Okay, so this time what are gonna be the input? Well, the first input is always the same. It's going to be, of course the set, you know, the data set on which you're gonna train your model here at the CNN, and that's of course the training set. And the name of the parameter for that is simply "X", and therefore we'll specify "X" to be, you know, our training set, that exact same training set which we created in part one, you know, right here, that training set, that training set to which we applied this image data generated tool, to indeed perform image augmentation. All right, so that training set, that's the first input of our parameter in this fit method. Okay, then next parameter.All right, so the next parameter is this time, you know the difference, the difference with what we did before. So it has to do, of course, with the fact that we are not only training the CNN on the training set, but also evaluating it at the same time on the test set. And that second parameter corresponds exactly to this, we have to specify here the validation data. That's the name of the parameter, but that's of course the set on which we want to evaluate our CNN. And that's of course the test set, that will be the value of the parameter. But the name of that parameter is, as I just said, "validation_data", and this is equal of course, to the test set. Once again, that exact same test set which we created here in part one when pre-processing a test set, this one to which of course, no transformation was applied, only feature scaling. Okay, so test set, good. And now we have one final argument, which is of course, you can totally guess what it is, right? This is the inevitable argument when training a deep neural network. I'm talking of course about the epochs parameter, which is a number of epochs. And well, you know, to justify to you which number I chose, well, I actually started with 10 epochs, and I noticed that the accuracy was not converging, so then I tried with 15 epochs, because you're gonna see that one epoch is actually pretty slow, you know it's actually a bit long, I mean much longer than the epochs in the training of our previous neural network, you know, the ANN. So I started with 10, it was not enough, then 15, still not enough, you know, still not converging. And then 25, and 25 was perfect. I had an accuracy that pretty much converged. not only on the training set,
cnn.fit(x = training_set, validation_data = test_set, epochs = 25)

# Part 4 - Making a single prediction

import numpy as np
from keras.preprocessing import image
test_image = image.load_img('dataset/dog.jpg', target_size = (64, 64)) # And then let's add the second argument, a very, very important one. And actually compulsory that image, which you know will become the input of the predict method has to have the same size as the one that was used during the training. And remember, we actually resized our images into, you know this size, target size of 64 by 64, you know, whether it was for the training set or the test set. And we specified this again when building the CNN right here with the input shape. So the size of any image we're gonna work with, whether it is to train our CNN or to call the predict method, has to be 64 by 64.
test_image = image.img_to_array(test_image)                            # So now we have a first test image, but in order to be accepted by the predict method, which, you know expect a certain format, we have to do some extra work here on that test image. And that first extra work is to convert that P I L, you know, PIL format, which is a format of images into an array. Remember that the predict method expects as its input a 2D array, you know? You remember these double pair of square brackets which we used to input in the predict methods of our previous models, you know, when predicting the outcome of a single observation. Well here we're about to do exactly the same by converting this test image into an array. And the way we're gonna do this is with another function of this image data progressing module, which is exactly this one image to array which converts indeed a P I L image instance into a non by array, which is exactly the format of array expected by the predict method. And guess what it has to take as input? Well, very simply, it just needs to take that image in the P I L format, which we want to convert into the non by array format.
test_image = np.expand_dims(test_image, axis = 0) # So that the next step. But then we still have some extra work to do which is still related to the fact that, you know, the predict method has to be called on the exact same format that was used during the training. And well remember that, you know when pre-processing our training set and our test set right here in part one data be processing we actually created batches of images. Therefore, you know, our CNN was not trained on single images, you know, entering the network one after the other, but it was trained with batches of images. And therefore we have this extra dimension corresponding to the batch. You know, we have batch number one containing 32 images then batch number two containing other 32 images batch number three, et cetera. So we have this extra dimension of the batch. And even if now we're about to deploy our model on a single image, well that single image still has to be into a batch. Even if we're gonna have one image in the batch it needs to be into this batch so that the CNN model, and more specifically the predict method of the CNN model, can recognize the batch as that extra dimension. And so you guessed it. What we're about to do now is just to add an extra dimension, which will correspond to the batch and which will contain that image into a batch. And the way to do this is once again by,you know I just copied it before so I can just paste it once again update our test image by adding to it this extra dimension, corresponding to the batch. And the way to do this is with NumPy, you know that's how we can always easily manipulate a NumPy array now that you know, test image is indeed a NumPy array. And so I'm gonna call NumPy first, which has a shortcut NP from which I'm gonna call this function which allows exactly to add a fake dimension or you know a dimension corresponding to the batch and which is called the X band on the score dims function. And now you guess, once again what we have to input inside this function. Well, we have to input the image to which we want to add this extra dimension corresponding to the batch. So first I have to paste, once again our test image. But we have to add one extra argument here, which is where we want to add that extra dimension. And the dimension of the batch is actually always the first dimension. You know, that makes sense. You gave first the batch of images and then inside each batch you get the different images. So it seems natural to have the batch as the first dimension. And to specify this that's exactly what we need to enter here as this extra argument and that parameter to specify this is access which we have to set equal to zero
result = cnn.predict(test_image) # because this predict method will return 0 or 1, So the way to figure out what is zero and what is one, you know if zero is cat or dog and one is cat or dog Well the trick is to call the class indices attribute from our training set object.
print(training_set.class_indices)
if result[0][0] == 1:            # So that's how we get exactly the prediction we want by first accessing the batch and then accessing the single element of the batch, we need to get inside this batch of index zero well the first and only element, you know, the first and only prediction, which has once again index zero.
    prediction = 'dog'
else:
    prediction = 'cat'
print(prediction)









